{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_LACV_BasicsLinearAlgebra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jYQE37rOc539"
      },
      "source": [
        "# Matemática para Ciencia de los Datos\n",
        "\n",
        "-Profesor: Luis Alexánder Calvo Valverde.\n",
        "\n",
        "- Documento base: Saúl Calderón, Žiga Emeršič, Ángel García, Blaž Meden, Felipe Meza, Martín Solís, Juan Esquivel, Mauro Méndez, Manuel Zumbado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ROgYfJVH5_8",
        "colab_type": "text"
      },
      "source": [
        "# Sistemas lineales \n",
        "\n",
        "Gran parte del curso se basará en el estudio de sistemas o modelos lineales para realizar desde el filtrado de una señal (ya sea para eliminar aspectos indeseados, o mejorar cualidades de importancia), hasta la construcción de modelos de clasificación. \n",
        "\n",
        "Aunque gran parte de los sistemas reales son no lineales, modelos aproximados lineales de tales sistemas facilitan su análisis. Se presentan entonces el concepto básico de linealidad, fundamental en el desarrollo del curso. \n",
        "\n",
        "Un sistema es lineal si su respuesta ante la suma de dos entradas cualesquiera es la suma de su respuesta a cada una de las entradas por separado "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chJbeF0dXmO",
        "colab_type": "text"
      },
      "source": [
        "# Linealidad\n",
        "\n",
        "Sea $L\\left\\{ \\cdot\\right\\}$  un operador, $f(x)$, $f\\left(x_{1}\\right)$ y $f\\left(x_{2}\\right)$ funciones de una variable $x\\in\\mathbb{R}$ (que en señales unidimensionales corresponde usualmente al tiempo), con los escalares $\\alpha\\in\\mathbb{R}$ y $\\beta\\in\\mathbb{R}$. \n",
        "\n",
        "Se dice que el operador $L$ es lineal si cumple con las propiedades de homogeneidad (también conocida como escalamiento) y superposición\n",
        "\n",
        "Homogeneidad (escalamiento) \n",
        "$$\n",
        " L\\{\\alpha f(x)\\}= \\alpha L\\{f(x)\\}\n",
        "$$\n",
        "Superposición\n",
        "$$\n",
        " L\\{f_1(x)+f_2(x)\\}= L\\{f_1(x)\\}+L\\{f_2(x)\\}\n",
        "$$\n",
        "\n",
        "Lo cual se puede resumir en una sola ecuación como: \n",
        "\n",
        "$$L\\left\\{ \\alpha f_{1}\\left(x\\right)+\\beta f_{2}\\left(x\\right)\\right\\} =\\alpha L\\left\\{ f_{1}(x)\\right\\} +\\beta L\\left\\{ f_{2}(x)\\right\\}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb2fBM_twC7Y",
        "colab_type": "text"
      },
      "source": [
        "## Ejemplos\n",
        "\n",
        "Sean los siguientes sistemas L cuya entrada es la función u(t) y la salida es g(t) con h(t) cualquiera.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZObH4Wa1LSya",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "*  $g(t)=5\\,u(t)$. Con una entrada dada por $\\alpha u_{1}(t)+\\beta u_{2}(t)$, se tiene que:\n",
        "\n",
        "$L\\left\\{ \\alpha f_{1}\\left(x\\right)+\\beta f_{2}\\left(x\\right)\\right\\}$  $=?$  $\\alpha L\\left\\{ f_{1}(x)\\right\\} +\\beta L\\left\\{ f_{2}(x)\\right\\}$\n",
        "\n",
        "$5\\left(\\alpha u_{1}(t)+\\beta u_{2}(t)\\right)$ $=?$ $\\alpha\\,5\\,u_{1}(t)+\\beta\\,5\\,u_{2}(t)$\n",
        "\n",
        "$5\\alpha\\,u_{1}(t)+5\\beta\\,u_{2}(t)$  $=?$  $\\alpha\\,5\\,u_{1}(t)+\\beta\\,5\\,u_{2}(t)$\n",
        "\n",
        "$\\alpha\\,5\\,u_{1}(t)+\\beta\\,5,u_{2}(t)=\\alpha\\,5\\,u_{1}(t)+\\beta\\,5\\,u_{2}(t)$\n",
        "\n",
        "\n",
        "por lo tanto el sistema es lineal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rosiWjBXWW9E"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   $g(t)=\\sqrt{u(t)}$. Con una entrada dada por $\\alpha u_{1}(t)+\\beta u_{2}(t)$, se tiene que: \n",
        "\n",
        "$L\\left\\{ \\alpha f_{1}\\left(x\\right)+\\beta f_{2}\\left(x\\right)\\right\\}$ $=$ $\\alpha L\\left\\{ f_{1}(x)\\right\\} +\\beta L\\left\\{ f_{2}(x)\\right\\}$\n",
        "\n",
        "\n",
        "$\\sqrt{\\left(\\alpha u_{1}(t)+\\beta u_{2}(t)\\right)}$ $=?$ $ \\alpha\\sqrt{u_{1}(t)}+\\beta\\sqrt{u_{2}(t)}$\n",
        "\n",
        "\n",
        "por lo tanto el sistema no es lineal. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vdkgDyPlWY2n"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   $g(t)=u(t)\\,\\cos\\left(\\omega t\\right)$. Con una entrada dada por $\\alpha u_{1}(t)+\\beta u_{2}(t)$, se tiene que: \n",
        "\n",
        "$L\\left\\{ \\alpha f_{1}\\left(x\\right)+\\beta f_{2}\\left(x\\right)\\right\\} =\\alpha L\\left\\{ f_{1}(x)\\right\\} +\\beta L\\left\\{ f_{2}(x)\\right\\}$\n",
        "\n",
        "$(\\alpha u_{1}(t)+\\beta u_{2}(t))\\cos\\left(\\omega t\\right)$ $=?$ $\\alpha u_{1}(t)\\cos\\left(\\omega t\\right)+\\beta u_{2}(t)\\cos\\left(\\omega t\\right) $\n",
        "\n",
        "$(\\alpha u_{1}(t)+\\beta u_{2}(t))\\cos\\left(\\omega t\\right)$ $=$ $(\\alpha u_{1}(t)+\\beta u_{2}(t))\\cos\\left(\\omega t\\right) $\n",
        "\n",
        "por lo tanto el sistema es lineal. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VaHi5JYXWgRB"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   $g(t)=\\frac{1}{1+\\exp\\left(-u(t)\\right)}$. Con una entrada dada por $\\alpha u_{1}(t)+\\beta u_{2}(t)$, se tiene que: \n",
        "\n",
        "$L\\left\\{ \\alpha f_{1}\\left(x\\right)+\\beta f_{2}\\left(x\\right)\\right\\}$ $=$ $\\alpha L\\left\\{ f_{1}(x)\\right\\} +\\beta L\\left\\{ f_{2}(x)\\right\\}$\n",
        "\n",
        "$\\frac{1}{1+\\exp\\left(-\\alpha u_{1}(t)-\\beta u_{2}(t)\\right)}$ $=?$  $\\frac{\\alpha}{1+\\exp\\left(-u_{1}(t)\\right)}+\\frac{\\beta}{1+\\exp\\left(-u_{2}(t)\\right)}$\n",
        "\n",
        "$\\frac{1}{1+\\exp\\left(-\\alpha u_{1}(t)\\right)\\exp\\left(-\\beta u_{2}(t)\\right)}$ $=?$  $\\frac{\\alpha\\left(1+\\exp\\left(-u_{2}(t)\\right)\\right)+\\beta\\left(1+\\exp\\left(-u_{1}(t)\\right)\\right)}{\\left(1+\\exp\\left(-u_{1}(t)\\right)\\right)\\left(1+\\exp\\left(-u_{2}(t)\\right)\\right)}$\n",
        "\n",
        "\n",
        "por lo que el sistema es no lineal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3uCtcT9FFQ_X"
      },
      "source": [
        "## Instalar dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yqk8N_AeFRFX",
        "outputId": "40da2e98-a16b-4ab3-f7f6-ca5b347c2da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Si no tiene instalado pytorch hay que hacerlo\n",
        "!pip install torch\n",
        "\n",
        "###############################################################################\n",
        "import torch as torch\n",
        "import math \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "###############################################################################\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrarwtFB60_N",
        "colab_type": "text"
      },
      "source": [
        "# Vectores: magnitud, dirección, norma, producto punto\n",
        "\n",
        "**Definición:** Un vector es una colección de $n$ números ordenados, comúnmente escritos como una columna:\n",
        "$$$$\n",
        "\n",
        "$$\n",
        "x = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$$$\n",
        "Los números $x_1, ... , x_n$ son coordenadas o componentes del vector $x$. \n",
        "$$$$\n",
        "Los componentes de un vector normalmente son números reales o complejos. En el primer caso $x_i \\in\\mathbb{R}^n$, en el segundo $x_i \\in \\mathbb{C}^n$.\n",
        "\n",
        "Los vectores también se pueden representar como líneas en espacio $n$-dimensional\n",
        "$$$$\n",
        "Dos líneas que describen un vector pueden tener la misma dirección y el mismo largo, pero originarse en puntos diferentes. La convención estándar es que puntos en $\\mathbb{R}^2$, por ejemplo, son dibujados en el plano de coordenadas desde el origen (0, 0) hasta el punto $A$, donde los puntos $x_1, x_2$ de $A$, representan los componentes.\n",
        "\n",
        "El largo de un vector se le puede encontrar en la literatura como: longitud,  magnitud o módulo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjkc4mSb5m04",
        "colab_type": "code",
        "outputId": "aeb95013-e65f-40b9-d384-4427ffca135b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.tensor( [1.,4.,5.,6.])\n",
        "print( x )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 4., 5., 6.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX5GLlF4659a",
        "colab_type": "text"
      },
      "source": [
        "# Operaciones con vectores\n",
        "\n",
        "**Definición**: El producto del vector $x$ por el escalar $\\alpha$ es un vector\n",
        "$$$$\n",
        "$$\n",
        "\\alpha x = \\alpha \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} = \n",
        "            \\begin{bmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ ... \\\\ \\alpha x_n\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$$$\n",
        "\n",
        "Los vectores $a$ and $b$ para los que existe un escalar $\\alpha$ que cumple $a = \\alpha b$ or $b = \\alpha a$, son llamados $\\mathbf{vectores~colineales}$. Estos vectores, como su nombre sugiere, se encuentran sobre líneas paralelas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u_RoE5Y58lI",
        "colab_type": "code",
        "outputId": "d113a7c1-de20-451c-ec5a-42097971afda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# producto de un escalar por un vector\n",
        "a =10\n",
        "print( a )\n",
        "x = torch.tensor( [1,4,5,6])\n",
        "print( x )\n",
        "ax = a * x\n",
        "print( ax )\n",
        "\n",
        "# vectores colineales\n",
        "a = torch.tensor( [1.,4.,5.,8.])\n",
        "b = torch.tensor( [10.,40.,50.,60.])\n",
        "alpha = 0.1\n",
        "print( alpha * b )\n",
        "comparacion = torch.eq( a , alpha * b) \n",
        "print( comparacion )\n",
        "# 1 es True y 0 es False\n",
        "if min(comparacion) == True:\n",
        "  print(\"Son colineales\")\n",
        "else:\n",
        "  print(\"No son colineales\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "tensor([1, 4, 5, 6])\n",
            "tensor([10, 40, 50, 60])\n",
            "tensor([1., 4., 5., 6.])\n",
            "tensor([ True,  True,  True, False])\n",
            "No son colineales\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVKibY9Hcq_R",
        "colab_type": "code",
        "outputId": "c3cb9fae-9761-4ee2-f0d0-4f484cfb7acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print( max( True, False)  )\n",
        "print(  4 / 10)\n",
        "print( 4.0 / 10.0 )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "0.4\n",
            "0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwxhkwGG65zy",
        "colab_type": "text"
      },
      "source": [
        "## Suma de vectores\n",
        "\n",
        "**Definición:** La suma de dos vectores $x$ y $y$ es un vector (aplica para la resta - )\n",
        "$$$$\n",
        "$$\n",
        " x + y = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} + \n",
        "         \\begin{bmatrix}y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{bmatrix} = \n",
        "         \\begin{bmatrix} x_1 + y_1 \\\\ x_2 + y_2 \\\\ ... \\\\ x_n + y_n \\end{bmatrix}\n",
        "$$\n",
        "$$$$\n",
        "**Atención!** Los vectores que se suman deben tener el mismo número de componentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmtNkXJrDn0I",
        "colab_type": "code",
        "outputId": "9d43e977-5063-4aca-a6d3-1bad661b1d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "x = torch.tensor( [1,4,5,6])\n",
        "y = torch.tensor( [10,40,50,60])\n",
        "z1 = x + y\n",
        "print( z1 )\n",
        "z2 = y - x\n",
        "print( z2 )\n",
        "\n",
        "# Otra forma\n",
        "z3 = torch.add(x, y) \n",
        "print(z3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([11, 44, 55, 66])\n",
            "tensor([ 9, 36, 45, 54])\n",
            "tensor([11, 44, 55, 66])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ3bycsH7EuL",
        "colab_type": "text"
      },
      "source": [
        "## Vector nulo\n",
        "\n",
        "**Definición:** El vector $0$ se define como el vector que causa $a + 0 = 0 + a = a$\n",
        "para cada vector $a$. Todos loc componentes del vector cero son igual a $0$. Cada vector $a$ tiene un vector opuesto $-a$ tal que $a + (-a) = 0$. La diferencia de dos vectores $a$ y $b$ es la suma $a + (-b)$ y se escribe de la misma forma que la operación básica para escalares $a - b$.\n",
        "$$$$\n",
        "$$\n",
        " a + 0 = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ ... \\\\ a_n\\end{bmatrix} + \n",
        "         \\begin{bmatrix} 0 \\\\ 0 \\\\ ... \\\\ 0\\end{bmatrix} = \n",
        "         \\begin{bmatrix} a_1 \\\\ a_2 \\\\ ... \\\\ a_n\\end{bmatrix} = 0 + a = a\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaGD4le-EoWG",
        "colab_type": "code",
        "outputId": "309df40a-8c63-4864-9b04-f6c37bb91c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "x = torch.tensor( [1.,4.,5.,6. , 4. , 3., 8.])\n",
        "print( x )\n",
        "largo = x.shape[0]\n",
        "nulo = torch.zeros( largo  )\n",
        "print( nulo )\n",
        "z = x + nulo\n",
        "print( z )\n",
        "s = torch.tensor( [ [1. , 3.] , [4. , 7.] , [6.0 ,  3.0] ])\n",
        "print( s.size() )\n",
        "print( x.size() )\n",
        "print( \"largo\", largo)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 4., 5., 6., 4., 3., 8.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([1., 4., 5., 6., 4., 3., 8.])\n",
            "torch.Size([3, 2])\n",
            "torch.Size([7])\n",
            "largo 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frWoMOaz7EcR",
        "colab_type": "text"
      },
      "source": [
        "## Combinación lineal de vectores\n",
        "\n",
        "Cuando el producto por un escalar es combinado en secuencia con operaciones con otros vectores, obtenemos combinaciones lineales.\n",
        "\n",
        "\n",
        "**Definición:** La combinación lineal de vectores $x$ y $y$ es la suma $\\alpha x + \\beta y$.\n",
        "\n",
        "$$\n",
        " \\alpha x + \\beta y = \\alpha \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} + \n",
        "       \\beta \\begin{bmatrix}y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{bmatrix}\n",
        "$$\n",
        "$$$$\n",
        "De manera similar, una combinación lineal de varios vectores puede ser construída en secuencia, e.g.\n",
        "$\\alpha a + \\beta b + · · · + \\zeta z$ es una combinación lineal de vectores $a, b, ... , z$.\n",
        "\n",
        "\n",
        "**Atención!** Todos los vectores en la combinación lineal deben tener la misma cantidad de componentes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDOeGiu0EkgC",
        "colab_type": "code",
        "outputId": "dc1e3fed-2639-49bd-dfc5-2ffdb0bdc826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.tensor( [1,4,5,6])\n",
        "y = torch.tensor( [10,40,50,60])\n",
        "alpha = 3\n",
        "beta = 7\n",
        "r =  ( alpha * x ) + (beta * y)\n",
        "print(r)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 73, 292, 365, 438])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ixDh1n7ELi",
        "colab_type": "text"
      },
      "source": [
        "## Producto escalar (producto punto, producto interno)\n",
        "\n",
        "\n",
        "**Definición:** El producto escalar de vectores\n",
        "\n",
        "$x = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix}$ e \n",
        "$y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{bmatrix}$ \n",
        "\n",
        "es el número $x \\cdot y = x_1y_1 + x_2y_2 + · · · + x_ny_n$.  \n",
        "\n",
        "\n",
        "**Atención!** Los vectores utilizados en el producto escalar deben tener el mismo número de componentes.\n",
        "\n",
        "$$$$\n",
        "\n",
        "Puede ser fácilmente verificado que el producto escalar tiene las siguientes propiedades:\n",
        "    \n",
        "$$$$\n",
        "$\\mathbf{Propiedad~1:}$ Conmutativa: $x · y = y · x$.\n",
        "$$$$\n",
        "$\\mathbf{Propiedad~2:}$ Distributiva: $x · (y + z) = x · y + x · z$.\n",
        "$$$$\n",
        "$\\mathbf{Propiedad~3:}$ Asociativa: $x · (\\alpha y) = \\alpha(x · y) = (\\alpha x) · y$.\n",
        "$$$$\n",
        "$\\mathbf{Propiedad~4:}$ Definición positiva: para cada vector $x$, $x \\cdot x \\ge 0$. If $x \\cdot x = 0$, entonces $x = 0$.\n",
        "$$$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LknCHIiutZoU",
        "colab_type": "code",
        "outputId": "9a273b3d-e4f1-4d2f-f248-956582f9cfcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.tensor( [1,4,5,6])\n",
        "y = torch.tensor( [5,7,5,6])\n",
        "r =  torch.dot( x ,  y )\n",
        "print(r)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRg8HuoN7Whu",
        "colab_type": "text"
      },
      "source": [
        "## Largo, norma\n",
        "En el conjunto $\\mathbb{R}^2$ se puede utilizar la regla de Pitágoras para calcular el largo de un vector. Los catetos están formados por el eje de las $x$, siguiendo la distancia indicada por el vector, y una línea paralela al eje $y$. De esta forma, se tiene un triángulo rectangular cuya hipotenusa es el vector en cuestión. Su largo, por ende, se puede calcular como $\\sqrt{x^2 + y^2}$.\n",
        "\n",
        "$$$$\n",
        "Para vectores en más dimensiones $\\mathbb{R}^N$, el largo del vector puede ser calculado de manera similar:\n",
        "$x_1^2 + x_2^2 + ... + x_n^2$, que puede ser expresado en forma de vector como $\\sqrt{x \\cdot x}$.\n",
        "    \n",
        "\n",
        "\n",
        "$$$$\n",
        "$\\mathbf{Definicion:}$ El largo del vector x es\n",
        "$ \\|x\\| = \\sqrt{x \\cdot x}$.\n",
        "$$$$\n",
        "\n",
        "\n",
        "$$$$\n",
        "$\\mathbf{Definicion:}$ El vector unitario tiene un largo de $1$.\n",
        "$$$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIgZCNaRMT7W",
        "colab_type": "text"
      },
      "source": [
        "**Norma**\n",
        "\n",
        "El concepto de largo (longitud, magnitud o módulo) visto anteriormente, se conoce como la distancia Eucliciana o norma $\\ell_{2}$, la cual se refiere al largo de un vector, como vimos. La norma euclidiana se puede reescribir como:\n",
        "\n",
        "$$\\left\\Vert x\\right\\Vert _{2}=\\sqrt{\\sum_{i=1}^{n}x_{i}^{2}}$$\n",
        "\n",
        "\n",
        "Generalizando la ecuación anterior como una norma $\\ell_{p}$, con $p\\geq1$, se tiene que:\n",
        "\n",
        "$$\\left\\Vert x\\right\\Vert _{p}=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1/p}$$\n",
        "\n",
        "A partir de tal definición general, se tiene la norma $\\ell_{1}$ también conocida como Manhattan o distancia de bloques: \n",
        "\n",
        "$$\\left\\Vert x\\right\\Vert _{1}=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|\\right)$$\n",
        "\n",
        "La norma $\\ell_{\\infty}$ se define entonces como: \n",
        "\n",
        "$$\\left\\Vert x\\right\\Vert _{\\infty}=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{\\infty}\\right)^{1/\\infty}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fruda2sTt9g4",
        "colab_type": "code",
        "outputId": "857700b4-617f-4182-f68f-ff0bf6f22f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "x = torch.tensor( [ 2. , 7., -3] )\n",
        "\n",
        "# Norma 2\n",
        "normaX = torch.norm(x,p=2)\n",
        "print(normaX)\n",
        "\n",
        "# Norma 3\n",
        "normaX = torch.norm(x,p=3)\n",
        "print(normaX)\n",
        "\n",
        "# Vector unitario\n",
        "x = torch.tensor( [1.,0.,0.] )\n",
        "normaX = torch.norm(x,p=2)\n",
        "print(normaX)\n",
        "print( math.sqrt( 62))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.8740)\n",
            "tensor(7.2304)\n",
            "tensor(1.)\n",
            "7.874007874011811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCl9eJ7k7YVZ",
        "colab_type": "text"
      },
      "source": [
        "## Ángulo entre dos vectores\n",
        "$$$$\n",
        "$\\mathbf{Teorema:}$ Los vectores $x$ y $y$ son ortogonales (o rectangulares) exactamente cuando  $x · y = 0$.\n",
        "$$$$\n",
        "$\\mathbf{Teorema:}$ Si denotamos el ángulo entre dos vectores $x$ y $y$ como $\\varphi$ entonces\n",
        "$$$$\n",
        "$$\\frac{x \\cdot y}{\\|x\\| \\cdot \\|y\\|} = \\cos \\varphi$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NImxSJuz2H36",
        "colab_type": "text"
      },
      "source": [
        "Función seno:\n",
        "sin(θ) = Opuesto / Hipotenusa\n",
        "\n",
        "Función coseno:\n",
        "cos(θ) = Adyacente / Hipotenusa\n",
        "\n",
        "Función tangente:\n",
        "tan(θ) = Opuesto / Adyacente\n",
        "\n",
        "Función secante:\n",
        "sec(θ) = Hipotenusa / Adyacente\t(=1/cos)\n",
        "\n",
        "Función cosecante:\n",
        "csc(θ) = Hipotenusa / Opuesto\t(=1/sin)\n",
        "\n",
        "Función cotangente:\n",
        "cot(θ) = Adyacente / Opuesto\t(=1/tan)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT2uKqHyxqQI",
        "colab_type": "code",
        "outputId": "e1cde00c-e668-4d7f-b0c1-b354a5b7be56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "x = torch.tensor( [3. , 4. ])\n",
        "y = torch.tensor( [4. , -3. ])\n",
        "y = torch.tensor( [6. , 8. ])\n",
        "r = torch.dot(x ,  y )\n",
        "print( r )\n",
        "\n",
        "# graficar\n",
        "origin = [0] \n",
        "plt.quiver(origin, origin, x, y, color=['r','b','g'], scale=40)\n",
        "plt.show()\n",
        "\n",
        "r = (x*y) / ( torch.norm(x) * torch.norm(y) )\n",
        "print( r )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(50.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGSe3PBo3gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3600, 0.6400])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_sp76k7clR",
        "colab_type": "text"
      },
      "source": [
        "## Dirección de un vector\n",
        "$$$$\n",
        "$\\mathbf{Teorema:}$ Un vector $x$ puede ser normalizado al dividir sus componentes por el largo $\\|x\\|$: $x_{norma} = \\frac{x}{\\|x\\|}$. \n",
        "El vector $x_{norma}$ también se llama vector unitario y representa la dirección del vector inicial.\n",
        "    \n",
        "$$$$\n",
        "Podemos escribir los componentes del vector unitario como cosenos direccionales: \n",
        "        \n",
        "$$x_{norma} = \\begin{bmatrix} \\frac{x_1}{\\|x\\|} \\\\ \\frac{x_2}{\\|x\\|} \\\\ \\frac{x_3}{\\|x\\|} \\end{bmatrix}\n",
        "= \\begin{bmatrix} \\cos \\alpha \\\\ \\cos \\beta \\\\ \\cos \\gamma \\end{bmatrix}, $$\n",
        "\n",
        "donde $\\alpha, \\beta, \\gamma$ son los ángulos direccionales definidos con respecto a los ejes base.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUOIzpHnLBRs",
        "colab_type": "code",
        "outputId": "9f140d51-fe3f-4ebb-9373-6f43ed8a4dd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "x = torch.tensor( [3. , 4. , 5.])\n",
        "normaX = torch.norm(x)\n",
        "print(normaX)\n",
        "r = x  / normaX\n",
        "print( r )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(7.0711)\n",
            "tensor([0.4243, 0.5657, 0.7071])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkToAidB7f0b",
        "colab_type": "text"
      },
      "source": [
        "## Producto vectorial (producto cruz)\n",
        "\n",
        "**Definición:** El producto de dos vectores $a$ y $b$ es otro vector: \n",
        "\n",
        "$$ a \\times b = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3\\end{bmatrix} \\times \n",
        "              \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\end{bmatrix} = \n",
        "              \\begin{bmatrix}a_2 b_3 - a_3 b_2 \\\\ a_3 b_1 - a_1 b_3 \\\\ a_1 b_2 - a_2 b_1\\end{bmatrix} $$\n",
        "\n",
        "\n",
        "El producto cruz $a \\times b$ es un vector que es perpendicular al plano en el que los vectores $a$ y $b$ se encuentran.\n",
        "\n",
        "Su largo es igual al paralelogramo que los vectores $a$ y $b$ contienen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6exHvUgLo5Q",
        "colab_type": "code",
        "outputId": "4c31a164-b606-42d4-8d62-dfe4f53db950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = torch.tensor( [1. , 3. , 4. ])\n",
        "y = torch.tensor( [4. , 2. , 1. ])\n",
        "r = torch.cross( x, y)\n",
        "print( r )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ -5.,  15., -10.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}